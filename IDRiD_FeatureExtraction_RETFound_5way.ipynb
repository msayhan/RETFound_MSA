{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aabed6c-171d-4199-a636-90f6bdc46391",
   "metadata": {},
   "source": [
    "## Import libraries and define useful things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac198d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max float : 1.7976931348623157e+308\n",
      "2.7.0\n",
      "Cuda available : False\n",
      "Number of GPUs : 0\n",
      "CUDA Version : None\n",
      "timm Version : 1.0.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/msa/Projects/RETFound_MSA/robustretfound_venv/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image as PIL_Image\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, average_precision_score, f1_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from scipy.special import softmax\n",
    "# from scipy.special import expit\n",
    "from scipy.spatial import distance\n",
    "\n",
    "import relplot as rp\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# sys.path.insert(1, '../RETFound_MAE/')\n",
    "# retfound_dir = os.path.dirname('../RETFound_MAE/')\n",
    "# sys.path.insert(1, retfound_dir) \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import models_vit \n",
    "from util.pos_embed import interpolate_pos_embed\n",
    "from timm.models.layers import trunc_normal_\n",
    "import util.lr_decay as lrd\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import transforms as T\n",
    "# from torchvision.transforms import v2 as T\n",
    "\n",
    "import timm\n",
    "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "\n",
    "print(f'Max float : {sys.float_info.max}')\n",
    "print(torch.__version__)\n",
    "print(f'Cuda available : {torch.cuda.is_available()}')\n",
    "print(f'Number of GPUs : {torch.cuda.device_count()}')\n",
    "print(f'CUDA Version : {torch.version.cuda}')\n",
    "print(f'timm Version : {timm.__version__}')\n",
    "\n",
    "has_gpu = torch.cuda.is_available()\n",
    "has_mps = torch.backends.mps.is_built() #getattr(torch, 'has_mps', False)\n",
    "device = 'mps' if torch.backends.mps.is_built() else 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "chkpt_dir = '../Projects/RETFound_MAE/RETFound_mae_natureCFP.pth'\n",
    "\n",
    "input_size = 224\n",
    "\n",
    "def prepare_model(chkpt_dir, arch='vit_large_patch16'):\n",
    "    # build model\n",
    "    model = models_vit.__dict__[arch](\n",
    "        img_size=input_size,\n",
    "        num_classes=5,\n",
    "        drop_path_rate=0,\n",
    "        global_pool=True,\n",
    "    )\n",
    "    # load model\n",
    "    checkpoint = torch.load(chkpt_dir, weights_only=False, map_location=device)\n",
    "    msg = model.load_state_dict(checkpoint['model'], strict=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8fbc3b1-6ac1-4bc5-a2d8-f4ddb08429dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata shape : (413, 5)\n",
      "Index(['Image name', 'Retinopathy grade', 'Risk of macular edema ',\n",
      "       'file_path', 'split'],\n",
      "      dtype='object')\n",
      "Metadata shape : (103, 5)\n",
      "Index(['Image name', 'Retinopathy grade', 'Risk of macular edema ',\n",
      "       'file_path', 'split'],\n",
      "      dtype='object')\n",
      "Metadata shape : (516, 5)\n",
      "Index(['Image name', 'Retinopathy grade', 'Risk of macular edema ',\n",
      "       'file_path', 'split'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# IDRiD \n",
    "img_dir_tr = '/Users/msa/Datasets/IDRiD/DiseaseGrading/OriginalImages/TrainingSet/crop_224/'\n",
    "# full_path_list_tr = sorted(glob.glob(img_dir_tr + '*' + '.jpg', recursive=False))\n",
    "# print(f'Number of files in {img_dir_tr}\\t{len(full_path_list_tr)}', flush=True)\n",
    "\n",
    "csv_file_tr = '/Users/msa/Datasets/IDRiD/DiseaseGrading/Groundtruths/TrainingLabels.csv'\n",
    "df_metadata_tr = pd.read_csv(csv_file_tr, low_memory=False)\n",
    "df_metadata_tr = df_metadata_tr[['Image name', 'Retinopathy grade', 'Risk of macular edema ']]\n",
    "file_paths = []\n",
    "split = []\n",
    "for idx, row in df_metadata_tr.iterrows():\n",
    "    file_paths.append(img_dir_tr + str(row['Image name']) + '.png') # '.jpg')\n",
    "    split.append('train')\n",
    "df_metadata_tr['file_path'] = file_paths\n",
    "df_metadata_tr['split'] = split\n",
    "print(f'Metadata shape : {df_metadata_tr.shape}')\n",
    "print(df_metadata_tr.columns)\n",
    "\n",
    "img_dir_te = '/Users/msa/Datasets/IDRiD/DiseaseGrading/OriginalImages/TestingSet/crop_224/'\n",
    "# full_path_list_te = sorted(glob.glob(img_dir_te + '*' + '.jpg', recursive=False))\n",
    "# print(f'Number of files in {img_dir_te}\\t{len(full_path_list_te)}', flush=True)\n",
    "\n",
    "csv_file_te = '/Users/msa/Datasets/IDRiD/DiseaseGrading/Groundtruths/TestingLabels.csv'\n",
    "df_metadata_te = pd.read_csv(csv_file_te, low_memory=False)\n",
    "file_paths = []\n",
    "split = []\n",
    "for idx, row in df_metadata_te.iterrows():\n",
    "    file_paths.append(img_dir_te + str(row['Image name']) + '.png') # '.jpg')\n",
    "    split.append('test')\n",
    "df_metadata_te['file_path'] = file_paths\n",
    "df_metadata_te['split'] = split\n",
    "print(f'Metadata shape : {df_metadata_te.shape}')\n",
    "print(df_metadata_te.columns)\n",
    "\n",
    "df_metadata = pd.concat([df_metadata_tr, df_metadata_te], axis=0)\n",
    "print(f'Metadata shape : {df_metadata.shape}')\n",
    "print(df_metadata.columns)\n",
    "\n",
    "del df_metadata_tr, df_metadata_te, file_paths, split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7726c0-ebd3-4edf-aec0-28f8cf3c9b1a",
   "metadata": {},
   "source": [
    "## Preprare RETFound and extract feature embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c044a415-1f83-495a-9702-b278f4d48e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision encoder model loaded.\n"
     ]
    }
   ],
   "source": [
    "from bazinga import IDRiD_ImageDataset\n",
    "\n",
    "chkpt_dir = './RETFound_mae_natureCFP.pth'\n",
    "vision_encoder = prepare_model(chkpt_dir, 'RETFound_mae')\n",
    "\n",
    "# device = torch.device('cuda')\n",
    "vision_encoder.to(device)\n",
    "print('Vision encoder model loaded.')\n",
    "\n",
    "transforms = T.Compose([\n",
    "    T.ToTensor(), \n",
    "    T.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD), \n",
    "])\n",
    "\n",
    "# transforms = T.Compose([\n",
    "#     # T.ToTensor(), #v1\n",
    "#     T.ToImage(),  # v2\n",
    "#     T.ToDtype(torch.uint8, scale=True), # v2\n",
    "#     T.ToDtype(torch.float32, scale=True), # v2\n",
    "#     T.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n",
    "# ])\n",
    "\n",
    "\n",
    "# class IDRiD_ImageDataset(Dataset):\n",
    "#     def __init__(self, metadata, target_column='Retinopathy grade', \n",
    "#                  transforms=None, target_transforms=None\n",
    "#                 ):\n",
    "#         self.metadata = metadata \n",
    "#         self.target_column = target_column        \n",
    "#         self.transforms = transforms\n",
    "#         self.target_transforms = target_transforms\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.metadata.shape[0]\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "\n",
    "#         filepath = self.metadata.iloc[idx]['file_path']\n",
    "#         with PIL_Image.open(filepath) as img:\n",
    "#             if len(img.size) < 3: # if single channel, convert to RGB\n",
    "#                 img = img.convert(mode='RGB')\n",
    "            \n",
    "#             if self.transforms:\n",
    "#                 img = self.transforms(img)\n",
    "        \n",
    "#         return img, int(self.metadata.iloc[idx][self.target_column])\n",
    "    \n",
    "#     # def get_labels(self):\n",
    "#     #     # return as series for ImbalancedDatasetSampler to read into a Pandas dataframe\n",
    "#     #     return self.metadata[self.target_column]\n",
    "\n",
    "\n",
    "num_workers = 8\n",
    "batch_size = 32\n",
    "\n",
    "# Note that shuffle is mutually exclusive with Sampler\n",
    "# shuffle_dict = {'train': False, 'test': False} #, 'test': False}\n",
    "\n",
    "idrid_dataset = IDRiD_ImageDataset(df_metadata, transforms=transforms, target_transforms=None)\n",
    "\n",
    "dataloader = DataLoader(idrid_dataset, batch_size=batch_size,\n",
    "                        shuffle=False, sampler=None, # samplers[split], \n",
    "                        num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa19e27a-f98e-46de-b290-92c261bca511",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/msa/Projects/RETFound_MSA/robustretfound_venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "052aa5028a1547609fd28b68b6c81616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features : (516, 1024)\n",
      "Labels : (516,), Unique labels : (array([0, 1, 2, 3, 4]), array([168,  25, 168,  93,  62]))\n"
     ]
    }
   ],
   "source": [
    "def extract_features(vision_encoder, dataloader):\n",
    "    \n",
    "    out_data = OrderedDict()\n",
    "    out_data['features'] = []\n",
    "    out_data['labels'] = []\n",
    "    \n",
    "    vision_encoder.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(iter(dataloader)):\n",
    "                    \n",
    "            inputs = inputs.to(device)\n",
    "            \n",
    "            # a dictionary of features from various read-out layers\n",
    "            # {readout_layer_name : features}\n",
    "            # with torch.autocast(device_type='cuda', dtype=torch.float16): #torch.cuda.amp.autocast():\n",
    "                # with torch.inference_mode(mode=True):\n",
    "            # outputs = model(inputs)\n",
    "            outputs = vision_encoder.forward_features(inputs)\n",
    "            outputs = torch.squeeze(outputs)\n",
    "            # for readout_layername, features in outputs.items():\n",
    "            outputs = np.squeeze(outputs.cpu().detach().numpy())\n",
    "            out_data['features'].append(outputs)\n",
    "            out_data['labels'].append(labels)\n",
    "            # break # only 1 readout layer name!!\n",
    "    \n",
    "    \n",
    "    # list to numpy array\n",
    "    out_data['features'] = np.concatenate(out_data['features'], axis=0) \n",
    "    out_data['labels'] = np.concatenate(out_data['labels'], axis=0) \n",
    "        \n",
    "    print(f'Features : {out_data[\"features\"].shape}') \n",
    "    print(f'Labels : {out_data[\"labels\"].shape}, Unique labels : {np.unique(out_data[\"labels\"], return_counts=True)}') \n",
    "    \n",
    "    return out_data\n",
    "\n",
    "out_data = extract_features(vision_encoder, dataloader)\n",
    "\n",
    "X, y = out_data['features'], np.asarray(out_data['labels'], dtype=np.int32)\n",
    "    \n",
    "with open(f'IDRiD_Features_MultiClass.npy', 'wb') as handle:\n",
    "    # pickle.dump(out_data, handle, protocol=4)\n",
    "    np.save(handle, out_data['features'])\n",
    "    np.save(handle, out_data['labels'])\n",
    "\n",
    "del out_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a856c99-6f30-4be4-b819-7ce4d434b802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3255814  0.04844961 0.3255814  0.18023256 0.12015504]\n"
     ]
    }
   ],
   "source": [
    "print(f'{np.unique(y, return_counts=True)[1]/np.sum(np.unique(y, return_counts=True)[1])}')"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m121"
  },
  "kernelspec": {
   "display_name": "robustretfound_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
