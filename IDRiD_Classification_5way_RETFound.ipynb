{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aabed6c-171d-4199-a636-90f6bdc46391",
   "metadata": {},
   "source": [
    "## Import libraries and define useful things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5901ed6f-87d4-4a54-9a5c-13531c5315c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max float : 1.7976931348623157e+308\n",
      "2.7.0\n",
      "Cuda available : False\n",
      "Number of GPUs : 0\n",
      "CUDA Version : None\n",
      "timm Version : 1.0.15\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image as PIL_Image\n",
    "\n",
    "import os, glob\n",
    "import gc\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, average_precision_score, f1_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from scipy.special import softmax\n",
    "# from scipy.special import expit\n",
    "from scipy.spatial import distance\n",
    "\n",
    "import relplot as rp\n",
    "\n",
    "# sys.path.insert(1, '../RETFound_MAE/')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import models_vit\n",
    "from util.pos_embed import interpolate_pos_embed\n",
    "# from timm.models.layers import trunc_normal_\n",
    "from timm.layers import trunc_normal_\n",
    "import util.lr_decay as lrd\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import transforms as T\n",
    "# from torchvision.transforms import v2 as T\n",
    "\n",
    "import timm\n",
    "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "\n",
    "print(f'Max float : {sys.float_info.max}')\n",
    "print(torch.__version__)\n",
    "print(f'Cuda available : {torch.cuda.is_available()}')\n",
    "print(f'Number of GPUs : {torch.cuda.device_count()}')\n",
    "print(f'CUDA Version : {torch.version.cuda}')\n",
    "print(f'timm Version : {timm.__version__}')\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "has_gpu = torch.cuda.is_available()\n",
    "has_mps = torch.backends.mps.is_built() #getattr(torch, 'has_mps', False)\n",
    "device = 'mps' if torch.backends.mps.is_built() else 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "chkpt_dir = './RETFound_mae_natureCFP.pth'\n",
    "model_name = 'RETFound_mae'\n",
    "input_size = 224\n",
    "num_classes=5\n",
    "\n",
    "def prepare_model(chkpt_dir, arch=model_name):\n",
    "    # build model\n",
    "    model = models_vit.__dict__[arch](\n",
    "        img_size=input_size,\n",
    "        num_classes=5,\n",
    "        drop_path_rate=0,\n",
    "        global_pool=True,\n",
    "    )\n",
    "    # load model\n",
    "    checkpoint = torch.load(chkpt_dir, weights_only=False, map_location=device)\n",
    "    msg = model.load_state_dict(checkpoint['model'], strict=False)\n",
    "    return model\n",
    "\n",
    "\n",
    "model_save_dir = './modelstore/IDRiD_FT_RETFound_multiclass/'\n",
    "model_descriptor = model_name + 'Nature_CFP_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8fbc3b1-6ac1-4bc5-a2d8-f4ddb08429dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata shape : (413, 5)\n",
      "Index(['Image name', 'Retinopathy grade', 'Risk of macular edema ',\n",
      "       'file_path', 'split'],\n",
      "      dtype='object')\n",
      "Metadata shape : (103, 5)\n",
      "Index(['Image name', 'Retinopathy grade', 'Risk of macular edema ',\n",
      "       'file_path', 'split'],\n",
      "      dtype='object')\n",
      "Metadata shape : (516, 5)\n",
      "Index(['Image name', 'Retinopathy grade', 'Risk of macular edema ',\n",
      "       'file_path', 'split'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# IDRiD \n",
    "img_dir_tr = '/Users/msa/Datasets/IDRiD/DiseaseGrading/OriginalImages/TrainingSet/crop_224/'\n",
    "# full_path_list_tr = sorted(glob.glob(img_dir_tr + '*' + '.jpg', recursive=False))\n",
    "# print(f'Number of files in {img_dir_tr}\\t{len(full_path_list_tr)}', flush=True)\n",
    "\n",
    "csv_file_tr = '/Users/msa/Datasets/IDRiD/DiseaseGrading/Groundtruths/TrainingLabels.csv'\n",
    "df_metadata_tr = pd.read_csv(csv_file_tr, low_memory=False)\n",
    "df_metadata_tr = df_metadata_tr[['Image name', 'Retinopathy grade', 'Risk of macular edema ']]\n",
    "file_paths = []\n",
    "split = []\n",
    "for idx, row in df_metadata_tr.iterrows():\n",
    "    file_paths.append(img_dir_tr + str(row['Image name']) + '.png') # '.jpg')\n",
    "    split.append('train')\n",
    "df_metadata_tr['file_path'] = file_paths\n",
    "df_metadata_tr['split'] = split\n",
    "print(f'Metadata shape : {df_metadata_tr.shape}')\n",
    "print(df_metadata_tr.columns)\n",
    "\n",
    "img_dir_te = '/Users/msa/Datasets/IDRiD/DiseaseGrading/OriginalImages/TestingSet/crop_224/'\n",
    "# full_path_list_te = sorted(glob.glob(img_dir_te + '*' + '.jpg', recursive=False))\n",
    "# print(f'Number of files in {img_dir_te}\\t{len(full_path_list_te)}', flush=True)\n",
    "\n",
    "csv_file_te = '/Users/msa/Datasets/IDRiD/DiseaseGrading/Groundtruths/TestingLabels.csv'\n",
    "df_metadata_te = pd.read_csv(csv_file_te, low_memory=False)\n",
    "file_paths = []\n",
    "split = []\n",
    "for idx, row in df_metadata_te.iterrows():\n",
    "    file_paths.append(img_dir_te + str(row['Image name']) + '.png') # '.jpg')\n",
    "    split.append('test')\n",
    "df_metadata_te['file_path'] = file_paths\n",
    "df_metadata_te['split'] = split\n",
    "print(f'Metadata shape : {df_metadata_te.shape}')\n",
    "print(df_metadata_te.columns)\n",
    "\n",
    "df_metadata = pd.concat([df_metadata_tr, df_metadata_te], axis=0)\n",
    "print(f'Metadata shape : {df_metadata.shape}')\n",
    "print(df_metadata.columns)\n",
    "\n",
    "del df_metadata_tr, df_metadata_te, file_paths, split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04c49a4d-3842-4dce-b008-1f7c88f5945f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3255814  0.04844961 0.3255814  0.18023256 0.12015504]\n",
      "(417, 5)\n",
      "(47, 5)\n",
      "(52, 5)\n",
      "(417, 5)\n",
      "(47, 5)\n",
      "(52, 5)\n",
      "(417, 5)\n",
      "(47, 5)\n",
      "(52, 5)\n",
      "(417, 5)\n",
      "(47, 5)\n",
      "(52, 5)\n",
      "(417, 5)\n",
      "(47, 5)\n",
      "(52, 5)\n",
      "(417, 5)\n",
      "(47, 5)\n",
      "(52, 5)\n",
      "(418, 5)\n",
      "(47, 5)\n",
      "(51, 5)\n",
      "(418, 5)\n",
      "(47, 5)\n",
      "(51, 5)\n",
      "(418, 5)\n",
      "(47, 5)\n",
      "(51, 5)\n",
      "(418, 5)\n",
      "(47, 5)\n",
      "(51, 5)\n"
     ]
    }
   ],
   "source": [
    "random_seed = 42\n",
    "#### df_metadata = df_metadata.sample(frac=1.0, random_state=random_seed) Bonkers!\n",
    "num_folds = 10\n",
    "# num_repeats = 5\n",
    "\n",
    "metadata_splits = []\n",
    "# Same order as in the .csv files and df_metadata\n",
    "with open(f'IDRiD_Features_MultiClass.npy', 'rb') as handle:\n",
    "    X = np.load(handle)\n",
    "    y = np.load(handle)\n",
    "\n",
    "print(f'{np.unique(y, return_counts=True)[1]/np.sum(np.unique(y, return_counts=True)[1])}')\n",
    "\n",
    "kFoldCV = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=random_seed)\n",
    "for i, (trainval_index, test_index) in enumerate(kFoldCV.split(X, y)):\n",
    "\n",
    "    X_trainval = X[trainval_index,:]\n",
    "    y_trainval = y[trainval_index]\n",
    "    df_metadata_trainval = df_metadata.iloc[trainval_index]\n",
    "\n",
    "    kFoldCV_inner = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=random_seed)\n",
    "    for ii, (train_index, val_index) in enumerate(kFoldCV_inner.split(X_trainval, y_trainval)):\n",
    "        break\n",
    "\n",
    "    metadata_dict = {\"train\": df_metadata_trainval.iloc[train_index], \n",
    "                     \"val\": df_metadata_trainval.iloc[val_index], \n",
    "                     \"test\": df_metadata.iloc[test_index]\n",
    "                    }\n",
    "    \n",
    "    print(metadata_dict['train'].shape)\n",
    "    print(metadata_dict['val'].shape)\n",
    "    print(metadata_dict['test'].shape)\n",
    "    \n",
    "    metadata_splits.append(metadata_dict)\n",
    "\n",
    "del metadata_dict, X_trainval, y_trainval, trainval_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7726c0-ebd3-4edf-aec0-28f8cf3c9b1a",
   "metadata": {},
   "source": [
    "## Preprare dataset, dataloaders, transformations and perform cross-validation with RETFound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c044a415-1f83-495a-9702-b278f4d48e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bazinga import IDRiD_ImageDataset\n",
    "\n",
    "transforms_train = T.Compose([\n",
    "    T.ToTensor(), #v1\n",
    "    # T.ToImage(),  # v2\n",
    "    # T.ToDtype(torch.uint8, scale=True), # v2\n",
    "    \n",
    "    # T.Resize(size=(input_size,input_size), interpolation=T.InterpolationMode.BILINEAR),\n",
    "    T.RandomResizedCrop(size=(input_size, input_size), scale=(0.9, 1.0), ratio=(0.9, 1.1), interpolation=T.InterpolationMode.BILINEAR), \n",
    "    \n",
    "    T.RandomApply([T.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.9),\n",
    "    T.RandomApply([T.GaussianBlur((23,23), sigma=(0.1, 2.0))], p=0.1),\n",
    "\n",
    "    # T.RandomGrayscale(p=0.25), \n",
    "    \n",
    "#     # Following the color transformations, spatial/geometric transformations are due.\n",
    "#     # T.RandomHorizontalFlip(p=0.25),\n",
    "#     # T.RandomVerticalFlip(p=0.25),    \n",
    "#     # use NEAREST. Others cause values outside [0,255]\n",
    "    T.RandomApply([T.RandomRotation(10, interpolation=T.InterpolationMode.BILINEAR)], p=0.9),\n",
    "    \n",
    "    # T.ToDtype(torch.float32, scale=True), # v2\n",
    "    T.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD), # [-1, 1]\n",
    "])\n",
    "\n",
    "transforms_inf = T.Compose([\n",
    "    T.ToTensor(), #v1\n",
    "    # T.ToImage(),  # v2\n",
    "    # T.ToDtype(torch.uint8, scale=True), # v2\n",
    "    \n",
    "    # T.Resize(size=(input_size,input_size), interpolation=T.InterpolationMode.BILINEAR),\n",
    "\n",
    "    # T.ToDtype(torch.float32, scale=True), # v2\n",
    "    T.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD), # [-1, 1]\n",
    "])\n",
    "\n",
    "transforms = {'train': transforms_train, 'val': transforms_inf, 'test': transforms_inf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8258d6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_workers = 8\n",
    "# batch_size = 1 # 32 // n_views #52 # 128 # 96 # 128 # 208 # 164 # 112 # 75 # 22*4\n",
    "\n",
    "# # Note that shuffle is mutually exclusive with Sampler\n",
    "# # shuffle_dict = {'train': False, 'test': False} #, 'test': False}\n",
    "\n",
    "# split = 'train'\n",
    "\n",
    "# idrid_dataset = IDRiD_ImageDataset(metadata_splits[0][split], transforms=transforms[split], target_transforms=None)\n",
    "\n",
    "# dataloader = DataLoader(idrid_dataset, batch_size=batch_size,\n",
    "#                         shuffle=False, sampler=None, # samplers[split], \n",
    "#                         num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "# width = 5\n",
    "# height = 5\n",
    "# n_rows = 5 # len(data_loaders)\n",
    "# n_cols = 5\n",
    "\n",
    "# f = plt.figure(figsize=(n_cols*width, n_rows*height))\n",
    "\n",
    "# loader = iter(dataloader)\n",
    "# print(f'Size : {len(idrid_dataset)}')\n",
    "\n",
    "# for i in range(n_rows): #, (split, loader) in enumerate(data_loaders.items()):\n",
    "    \n",
    "#     for j in range(n_cols):\n",
    "\n",
    "#         img, label = next(loader) # cfp and oct views packed together\n",
    "        \n",
    "#         idx = (i*n_cols)+j\n",
    "        \n",
    "#         ax = f.add_subplot(n_rows, n_cols, idx+1)\n",
    "#         img = torch.squeeze(img)\n",
    "#         temp_img = torch.squeeze(img.permute(1,2,0))\n",
    "#         print(f'Min : {torch.amin(temp_img)}\\tMean : {temp_img.mean((0,1))}\\tStd : {temp_img.std((0,1))}\\tMax : {torch.amax(temp_img)}')\n",
    "\n",
    "#         ax.imshow(temp_img)\n",
    "        \n",
    "#         ax.set_title(f'{label.item()}')\n",
    "#         ax.set_xlabel('')\n",
    "#         ax.set_ylabel('')\n",
    "#         ax.set_xticks([])\n",
    "#         ax.set_xticklabels([])\n",
    "#         ax.set_yticks([])\n",
    "#         ax.set_yticklabels([])\n",
    "\n",
    "# # plt.savefig('../../retfoundm_images_multiview.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa19e27a-f98e-46de-b290-92c261bca511",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "### My generic model architecture for adapting RETFound to downstream tasks ###\n",
    "#################################################################################\n",
    "class DownstreamTask_Network(nn.Module):\n",
    "    def __init__(self, backbone, num_outputs, frozen_backbone=True, hidden_dims=[1024, 2048, 512]):\n",
    "        super(DownstreamTask_Network, self).__init__()\n",
    "        \n",
    "        self.backbone = backbone #.fullstack\n",
    "        if frozen_backbone:\n",
    "            print(f'Freezing the backbone parameters')\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.num_outputs = num_outputs\n",
    "        self.hidden_dims = hidden_dims # dim[0] corresponds to the input layer!\n",
    "        \n",
    "        # reconstruct the avg. pooling layer before head\n",
    "        # self.backbone.append(nn.Sequential(nn.AdaptiveAvgPool2d(1), \n",
    "        #                                    nn.LayerNorm([self.hidden_dims[0],1,1]), \n",
    "        #                                    nn.Flatten(start_dim=1, end_dim=-1)\n",
    "        #                                   )\n",
    "        #                     )        \n",
    "        \n",
    "        # self.head = nn.Sequential()\n",
    "        modules = []\n",
    "        if len(self.hidden_dims) > 1: # hidden layers exist\n",
    "            for idx in range(len(self.hidden_dims)-1):\n",
    "                modules.append(nn.Linear(self.hidden_dims[idx], self.hidden_dims[idx+1]))\n",
    "                modules.append(nn.LayerNorm(self.hidden_dims[idx+1]))\n",
    "                modules.append(nn.GELU(approximate='tanh'))\n",
    "\n",
    "        modules.append(nn.Linear(self.hidden_dims[-1], self.num_outputs))\n",
    "        \n",
    "        self.head = nn.Sequential(*modules)\n",
    "    \n",
    "    def forward_features(self, x):\n",
    "        return self.backbone.forward_features(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.head(self.forward_features(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a856c99-6f30-4be4-b819-7ce4d434b802",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintCallback:\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print(f\"Epoch {epoch}: loss[train] = {logs['loss']['train']:.5f}\\tloss[val] = {logs['loss']['val']:.5f}\") #, accuracy = {logs['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a5468e-b40d-4e65-ab50-da1b864aec3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs, batch_accumulation_steps, device, callbacks=None, \n",
    "                model_save_dir='../../', model_descriptor='IDRiD_RETFound_'):\n",
    "    \n",
    "    if model_save_dir is not None and not os.path.exists(model_save_dir):\n",
    "        os.makedirs(model_save_dir, exist_ok=True)\n",
    "    \n",
    "    total_loss_history = {'train' : [], 'val' : []}\n",
    "    best_loss = {'train' : sys.float_info.max, 'val' : sys.float_info.max}\n",
    "        \n",
    "    # Create once at the beginning of training\n",
    "    # scaler = torch.amp.GradScaler(device=device)\n",
    "    # log_softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}', flush=True)\n",
    "        print('-' * 10, flush=True)\n",
    "\n",
    "        epoch_loss = {'train' : 0.0, 'val' : 0.0}\n",
    "        logs = {'loss': epoch_loss}\n",
    "    \n",
    "        for phase in ['train','val']:\n",
    "\n",
    "            print(f'# of examples for \"{phase}\" : {len(dataloaders[phase].dataset)}', flush=True)\n",
    "\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = [] \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            batch_idx = 0 \n",
    "            \n",
    "            for datum in tqdm(iter(dataloaders[phase])): # Each is of shape B x K x 3 x H x W\n",
    "                # Casts operations to mixed precision\n",
    "                # forward\n",
    "                # with torch.autocast(device_type=device, dtype=torch.float16): #torch.cuda.amp.autocast():\n",
    "                with torch.set_grad_enabled(phase == 'train'): \n",
    "\n",
    "                    datum[0] = datum[0].to(device) # img\n",
    "                    # print(f'{datum[0].type()}')\n",
    "                    datum[1] = datum[1].to(device) # label\n",
    "\n",
    "                    output = model(datum[0]) #(torch.unsqueeze(datum[0], dim=0))\n",
    "\n",
    "                    # features = model.forward_features(datum[0])\n",
    "                    # print(torch.squeeze(features.cpu().detach()).numpy())\n",
    "                    # print(torch.squeeze(output.cpu().detach()).numpy())\n",
    "\n",
    "                    # loss = criterion(nn.functional.softmax(output, dim=-1), nn.functional.one_hot(datum[1].long(), num_classes=num_classes)) #torch.unsqueeze(datum[1].float(), dim=1))\n",
    "                    loss = criterion(output, datum[1].long()) #torch.unsqueeze(datum[1].float(), dim=1))\n",
    "                    # loss = criterion(log_softmax(output), nn.functional.one_hot(datum[1].long(), num_classes=num_classes))\n",
    "                    running_loss.append(loss.item())\n",
    "                    print(f'Batch loss : {loss.item()}')\n",
    "                    \n",
    "                    # # .backward() here accumulates gradients\n",
    "                    if phase == 'train':\n",
    "                        loss.backward() # do not call .backward within autocast. it is not recommended. \n",
    "                        # scaler.scale(loss).backward()\n",
    "                \n",
    "                if phase == 'train' and ((batch_idx+1)%batch_accumulation_steps==0):\n",
    "                    optimizer.step()\n",
    "                    # scaler.step(optimizer) # a step in the direction of gradients accumulated so far\n",
    "                    # # Updates the scale for next iteration\n",
    "                    # scaler.update()\n",
    "                    \n",
    "                    # scheduler.step()\n",
    "                    \n",
    "                    optimizer.zero_grad(set_to_none=True)\n",
    "                                \n",
    "                batch_idx += 1\n",
    "                \n",
    "            epoch_loss[phase] = np.sum(np.asarray(running_loss, dtype=np.float32)) / (len(dataloaders[phase].dataset))\n",
    "            total_loss_history[phase].append(epoch_loss[phase])\n",
    "\n",
    "            print(f'Epoch {epoch+1}, {phase} Loss : {epoch_loss[phase]:.4f}', flush=True)\n",
    "                            \n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss[phase] < best_loss[phase]: \n",
    "                \n",
    "                print(f'Better validation loss found : {epoch_loss[\"val\"]}\\t previous : {best_loss[\"val\"]}', flush=True)\n",
    "                \n",
    "                best_loss['train'] = epoch_loss['train']\n",
    "                best_loss['val'] = epoch_loss['val']\n",
    "            \n",
    "                checkpoint_path = f'{model_save_dir}{model_descriptor}.tar'\n",
    "                torch.save({'epoch': epoch,\n",
    "                            'model_state_dict': model.state_dict(),\n",
    "                            # 'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'loss': epoch_loss[phase],\n",
    "                           }, checkpoint_path)\n",
    "        \n",
    "        if callbacks is not None:\n",
    "            for callback in callbacks:\n",
    "                callback.on_epoch_end(epoch+1, logs)\n",
    "    \n",
    "    print('Best validation loss : {0:.4f}\\t found at epoch : {1:d}'.format(np.min(total_loss_history['val']), np.argmin(total_loss_history['val'])), flush=True)\n",
    "    \n",
    "    return model, total_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a74a34f2-457e-4d61-832a-cd9355c88b02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size : 8\t batch accumulation steps : 1\n",
      "Cross validation idx : 0\tTraining split size : 417\tValidation split size : 47\tTest split size : 52\n",
      "# of parameters in model : 303311882\n",
      "# of trainable parameters in model : 303311882\n",
      "Metadata split shape : (417, 5)\n",
      "Metadata split shape : (47, 5)\n",
      "Epoch 1/20\n",
      "----------\n",
      "# of examples for \"train\" : 417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/msa/Projects/RETFound_MSA/robustretfound_venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5625ab8b0b24624a866a7fead7d81ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 69\u001b[39m\n\u001b[32m     64\u001b[39m     dataloaders[phase] = DataLoader(datasets[phase], batch_size=batch_size,\n\u001b[32m     65\u001b[39m                                     shuffle=shuffle_dict[phase], sampler=\u001b[38;5;28;01mNone\u001b[39;00m, \n\u001b[32m     66\u001b[39m                                     num_workers=num_workers, pin_memory=\u001b[38;5;28;01mTrue\u001b[39;00m, drop_last=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# train the model along with model selection based on validation resutls, e.g., validation loss\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m model, total_loss_history = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m                                        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_accumulation_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mPrintCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m                                        \u001b[49m\u001b[43mmodel_save_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_descriptor\u001b[49m\u001b[43m+\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m loss_histories.append(total_loss_history)\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m backbone, model\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, dataloaders, criterion, optimizer, scheduler, num_epochs, batch_accumulation_steps, device, callbacks, model_save_dir, model_descriptor)\u001b[39m\n\u001b[32m     42\u001b[39m datum[\u001b[32m0\u001b[39m] = datum[\u001b[32m0\u001b[39m].to(device) \u001b[38;5;66;03m# img\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# print(f'{datum[0].type()}')\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m datum[\u001b[32m1\u001b[39m] = \u001b[43mdatum\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# label\u001b[39;00m\n\u001b[32m     46\u001b[39m output = model(datum[\u001b[32m0\u001b[39m]) \u001b[38;5;66;03m#(torch.unsqueeze(datum[0], dim=0))\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# features = model.forward_features(datum[0])\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# print(torch.squeeze(features.cpu().detach()).numpy())\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# print(torch.squeeze(output.cpu().detach()).numpy())\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead."
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "# verbose = False\n",
    "\n",
    "num_workers = 8\n",
    "batch_size = 8\n",
    "batch_accumulation_steps = 1\n",
    "print(f'Batch size : {batch_size}\\t batch accumulation steps : {batch_accumulation_steps}')\n",
    "# criterion = nn.BCEWithLogitsLoss(reduction='sum')\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "# criterion = nn.NLLLoss(reduction='sum')\n",
    "\n",
    "#SCHEDULER FOR COSINE DECAY\n",
    "T_0 = 100 # 100 # 10\n",
    "T_mult = 1 # 1 # 2\n",
    "eta_min = 1e-5\n",
    "\n",
    "base_lr = 1e-3 #5e-4 # 0.001\n",
    "lr = base_lr * (float(batch_size*batch_accumulation_steps)/256) # base learning rate w.r.t. effective batch size\n",
    "weight_decay = 1e-2\n",
    "layer_decay = 0.75\n",
    "\n",
    "# Note that shuffle is mutually exclusive with Sampler\n",
    "shuffle_dict = {'train': True, 'val': False, 'test': False}\n",
    "\n",
    "frozen_backbone = False\n",
    "\n",
    "loss_histories = []\n",
    "\n",
    "for cv_idx, metadata in enumerate(metadata_splits):\n",
    "    \n",
    "    print(f'Cross validation idx : {cv_idx}\\tTraining split size : {metadata[\"train\"].shape[0]}\\tValidation split size : {metadata[\"val\"].shape[0]}\\tTest split size : {metadata[\"test\"].shape[0]}')\n",
    "    \n",
    "    dim_mlp = 1024\n",
    "    \n",
    "    backbone = prepare_model(chkpt_dir, model_name)\n",
    "    # no_weight_decay_list = backbone.no_weight_decay()\n",
    "    model = DownstreamTask_Network(backbone, num_outputs=num_classes, frozen_backbone=frozen_backbone, hidden_dims=[dim_mlp])\n",
    "        \n",
    "    param_count = sum([p.numel() for p in model.parameters()])\n",
    "    print(f'# of parameters in model : {param_count}')\n",
    "    param_count = sum([p.numel() for p in model.parameters() if p.requires_grad])\n",
    "    print(f'# of trainable parameters in model : {param_count}')\n",
    "    model = model.float()\n",
    "    model = model.to(device)\n",
    "    # print(model)\n",
    "    \n",
    "    # set up the optimization objects\n",
    "    # param_groups = lrd.param_groups_lrd(model, weight_decay, no_weight_decay_list=no_weight_decay_list, layer_decay=layer_decay)\n",
    "    optimizer = torch.optim.AdamW([params for params in model.parameters() if params.requires_grad],\n",
    "                                  lr=lr, \n",
    "                                  betas=(0.9, 0.999), eps=1e-08, \n",
    "                                  weight_decay=weight_decay, amsgrad=False \n",
    "                                 )\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0, T_mult, eta_min=eta_min, last_epoch=-1)\n",
    "    \n",
    "    # set up the datasets and loaders for training and validation splits\n",
    "    datasets = OrderedDict()\n",
    "    dataloaders = OrderedDict()\n",
    "    for phase in ['train','val']:        \n",
    "        print(f'Metadata split shape : {metadata[phase].shape}', flush=True)\n",
    "        \n",
    "        datasets[phase] = IDRiD_ImageDataset(metadata[phase], target_column='Retinopathy grade', \n",
    "                                             transforms=transforms[phase], target_transforms=None)\n",
    "        dataloaders[phase] = DataLoader(datasets[phase], batch_size=batch_size,\n",
    "                                        shuffle=shuffle_dict[phase], sampler=None, \n",
    "                                        num_workers=num_workers, pin_memory=True, drop_last=False)\n",
    "    \n",
    "    # train the model along with model selection based on validation resutls, e.g., validation loss\n",
    "    model, total_loss_history = train_model(model, dataloaders, criterion, optimizer, scheduler, \n",
    "                                            num_epochs, batch_accumulation_steps, device, [PrintCallback()],\n",
    "                                            model_save_dir, model_descriptor+str(cv_idx))\n",
    "\n",
    "    loss_histories.append(total_loss_history)\n",
    "\n",
    "    del backbone, model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.clear_autocast_cache()\n",
    "    \n",
    "    # break # cross-val.\n",
    "\n",
    "# with open(f'{model_save_dir}{model_descriptor}LossHist.pkl', 'wb') as handle:\n",
    "#     pickle.dump(loss_histories, handle, protocol=4)\n",
    "\n",
    "del loss_histories\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfc0475-9dbd-48fb-8fd1-18a0a5d6c2b8",
   "metadata": {},
   "source": [
    "## Evaluate the models on the same splits as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e88c2d6-df70-4955-8743-59961d2c8f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the models for inference on the same partitions as above.\n",
    "# for both training and validation splits, calculate loss as follows:\n",
    "#    for each prediction head, sum the loss over minibatches and divide by the total sample size\n",
    "\n",
    "# criterion = nn.BCEWithLogitsLoss(reduction='sum')\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "# with open(f'{model_save_dir}{model_descriptor}LossHist.pkl', 'rb') as handle:\n",
    "#     loss_histories = pickle.load(handle)\n",
    "\n",
    "# print(f'Reading loss histories from {len(loss_histories)}-fold cross validation')\n",
    "\n",
    "verbose = False\n",
    "\n",
    "num_workers = 8\n",
    "batch_size = 8 #128\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Device : {device}')\n",
    "\n",
    "avg_losses = []\n",
    "performance_metrics = []\n",
    "\n",
    "df_results = pd.DataFrame()\n",
    "acc_col = []\n",
    "roc_auc_col = []\n",
    "avg_prec_col = []\n",
    "f1_col = []\n",
    "calib_error_col = []\n",
    "cv_col = []\n",
    "split_col = []\n",
    "sampling_col = []\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "\n",
    "for cv_idx, metadata in enumerate(metadata_splits):\n",
    "    \n",
    "    print(f'Cross validation idx : {cv_idx}\\tTraining split size : {metadata[\"train\"].shape[0]}\\tValidation split size : {metadata[\"val\"].shape[0]}\\tTest split size : {metadata[\"test\"].shape[0]}')\n",
    "    \n",
    "    dim_mlp = 1024\n",
    "    \n",
    "    backbone = prepare_model(chkpt_dir, 'vit_large_patch16')    \n",
    "    model = DownstreamTask_Network(backbone, num_outputs=1, frozen_backbone=False, hidden_dims=[dim_mlp])\n",
    "    \n",
    "    checkpoint_path = f'{model_save_dir}{model_descriptor+str(cv_idx)}.tar'\n",
    "    print(f'Loading : {checkpoint_path}')\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    del checkpoint\n",
    "\n",
    "    model = model.to(device)\n",
    "    # print(model)\n",
    "        \n",
    "    # set up the datasets and loaders for training and validation splits\n",
    "    # datasets = OrderedDict()\n",
    "    # dataloaders = OrderedDict()\n",
    "    avg_losses.append(OrderedDict())\n",
    "    performance_metrics.append(OrderedDict())\n",
    "\n",
    "    for phase in ['test']: #['train','val','test']:        \n",
    "        print(f'Metadata split shape : {metadata[phase].shape}', flush=True)\n",
    "\n",
    "        dataset = IDRiD_ImageDataset(metadata[phase], target_column='Retinopathy grade', \n",
    "                                     transforms=transforms[phase], target_transforms=None)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size,\n",
    "                                shuffle=False, sampler=None, \n",
    "                                num_workers=num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "        print(f'# of examples for \"{phase}\" : {len(dataloader.dataset)}', flush=True)\n",
    "        \n",
    "        model.eval()   # Set model to evaluate mode\n",
    "\n",
    "        running_loss = [] \n",
    "        predictive_prob = [] \n",
    "        labels = [] \n",
    "        performance_metrics[-1][phase] = OrderedDict()\n",
    "        batch_idx = 0 \n",
    "\n",
    "        for datum in tqdm(iter(dataloader)): # Each is of shape B x K x 3 x H x W\n",
    "\n",
    "            # Casts operations to mixed precision\n",
    "            # forward\n",
    "            # with torch.autocast(device_type='cuda', dtype=torch.float16): #torch.cuda.amp.autocast():\n",
    "\n",
    "            with torch.set_grad_enabled(False): \n",
    "                \n",
    "                datum[0] = datum[0].to(device) # img\n",
    "                datum[1] = datum[1].to(device) # label\n",
    "                \n",
    "                output = model(datum[0]) #(torch.unsqueeze(datum[0], dim=0))\n",
    "                \n",
    "                loss = criterion(output, torch.unsqueeze(datum[1], dim=1).float())\n",
    "                running_loss.append(loss.item())\n",
    "                # running_loss.append(np.sum(np.abs(np.subtract(output.cpu().detach().numpy(), datum[1].cpu().detach().numpy()))))\n",
    "                \n",
    "                predictive_prob.append(nn.functional.sigmoid(output).cpu().detach().numpy())\n",
    "                labels.append(datum[1].cpu().detach().numpy())\n",
    "                    \n",
    "\n",
    "            # # if ((batch_idx+1)%(2*batch_accumulation_steps)==0):\n",
    "            # if batch_idx % 10 == 0:\n",
    "            #     print(f'Batch idx : {batch_idx}\\tLoss : {running_loss[-1]}', flush=True)\n",
    "            \n",
    "            batch_idx += 1\n",
    "            \n",
    "        print(f'{np.sum(np.asarray(running_loss, dtype=np.float32))} / {len(dataloader.dataset)}')\n",
    "        avg_losses[-1][phase] = np.sum(np.asarray(running_loss, dtype=np.float32)) / len(dataloader.dataset)\n",
    "        \n",
    "        predictive_prob = np.concatenate(predictive_prob, axis=0)\n",
    "        labels = np.concatenate(labels, axis=0)\n",
    "        predictions = np.asarray((predictive_prob >= 0.5), dtype=np.int64)\n",
    "\n",
    "        all_predictions.append(predictions)\n",
    "        all_labels.append(labels)\n",
    "        \n",
    "        acc = accuracy_score(labels, predictions)\n",
    "        performance_metrics[-1][phase]['accuracy'] = acc\n",
    "        print(f'Accuracy : {acc}')\n",
    "        \n",
    "        roc_auc = roc_auc_score(labels, predictive_prob)\n",
    "        performance_metrics[-1][phase]['roc-auc'] = roc_auc\n",
    "        print(f'ROC-AUC : {roc_auc}')\n",
    "\n",
    "        avg_prec = average_precision_score(labels, predictive_prob)\n",
    "        performance_metrics[-1][phase]['avg-prec'] = avg_prec\n",
    "        print(f'Avg. Prec. : {avg_prec}')\n",
    "\n",
    "        f1 = f1_score(labels, predictions)\n",
    "        performance_metrics[-1][phase]['f1'] = f1\n",
    "        print(f'F1 : {f1}')\n",
    "\n",
    "        calib_error = rp.smECE(np.squeeze(predictive_prob), labels)\n",
    "        performance_metrics[-1][phase]['ECE'] = calib_error\n",
    "        print(f'ECE : {calib_error}')\n",
    "\n",
    "\n",
    "        acc_col.append(acc)\n",
    "        roc_auc_col.append(roc_auc)\n",
    "        avg_prec_col.append(avg_prec)\n",
    "        f1_col.append(f1)\n",
    "        calib_error_col.append(calib_error)\n",
    "        cv_col.append(cv_idx)\n",
    "        split_col.append(phase)\n",
    "        sampling_col.append(\"RETFound\")\n",
    "        \n",
    "        \n",
    "    # for phase in ['train','val']:\n",
    "    #     print(f'-- Task losses {phase} : {avg_losses[-1][phase]} --')\n",
    "    # print(f'Loss [train] : {avg_losses[-1][\"train\"]}\\t[val] : {avg_losses[-1][\"val\"]}\\nAccuracy [train] : {performance_metrics[-1][\"train\"][\"accuracy\"]}\\t[val] : {performance_metrics[-1][\"val\"][\"accuracy\"]}\\nROC-AUC [train] : {performance_metrics[-1][\"train\"][\"roc-auc\"]}\\t[val] : {performance_metrics[-1][\"val\"][\"roc-auc\"]}\\nAvg. Prec. [train] : {performance_metrics[-1][\"train\"][\"avg-prec\"]}\\t[val] : {performance_metrics[-1][\"val\"][\"avg-prec\"]}\\nF1 [train] : {performance_metrics[-1][\"train\"][\"f1\"]}\\t[val] : {performance_metrics[-1][\"val\"][\"f1\"]}')\n",
    "    # print(f'Loss [train] : {avg_losses[-1][\"train\"]}\\t[val] : {avg_losses[-1][\"val\"]}\\t[test] : {avg_losses[-1][\"test\"]}\\nAccuracy [train] : {performance_metrics[-1][\"train\"][\"accuracy\"]}\\t[val] : {performance_metrics[-1][\"val\"][\"accuracy\"]}\\t[test] : {performance_metrics[-1][\"test\"][\"accuracy\"]}\\nROC-AUC [train] : {performance_metrics[-1][\"train\"][\"roc-auc\"]}\\t[val] : {performance_metrics[-1][\"val\"][\"roc-auc\"]}\\t[test] : {performance_metrics[-1][\"test\"][\"roc-auc\"]}\\nAvg. Prec. [train] : {performance_metrics[-1][\"train\"][\"avg-prec\"]}\\t[val] : {performance_metrics[-1][\"val\"][\"avg-prec\"]}\\t[test] : {performance_metrics[-1][\"test\"][\"avg-prec\"]}\\nF1 [train] : {performance_metrics[-1][\"train\"][\"f1\"]}\\t[val] : {performance_metrics[-1][\"val\"][\"f1\"]}\\t[test] : {performance_metrics[-1][\"test\"][\"f1\"]}')\n",
    "\n",
    "    del backbone, model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # break # cross-val.\n",
    "\n",
    "# with open(f'{model_save_dir}{model_descriptor}CrossValPerformance.pkl', 'wb') as handle:\n",
    "#     pickle.dump([avg_losses, \n",
    "#                  performance_metrics], handle, protocol=4)\n",
    "\n",
    "# del avg_losses, performance_metrics\n",
    "\n",
    "\n",
    "df_results['Fold'] = cv_col\n",
    "df_results['Split'] = split_col\n",
    "df_results['Sampling'] = sampling_col\n",
    "df_results['Accuracy'] = acc_col\n",
    "df_results['ROC-AUC'] = roc_auc_col\n",
    "df_results['Avg. Precision'] = avg_prec_col\n",
    "df_results['F1 Score'] = f1_col\n",
    "df_results['ECE'] = calib_error_col\n",
    "\n",
    "\n",
    "# df_results.to_csv('ICL4Ophthalmology_IDRiD_Binary_RETFound_3splits_FIXED.csv', index=False)\n",
    "\n",
    "\n",
    "print('Done!')\n",
    "\n",
    "all_predictions = np.concatenate(all_predictions, axis=0)\n",
    "print(f'All predictions shape : {all_predictions.shape}')\n",
    "print(all_predictions)\n",
    "all_labels = np.concatenate(all_labels, axis=0)\n",
    "print(f'All labels shape : {all_labels.shape}')\n",
    "print(all_labels)\n",
    "\n",
    "\n",
    "# with open(f'IDRiD_Binary_PredictionsRETFound_FIXED.npy', 'wb') as handle:\n",
    "#     # pickle.dump(out_data, handle, protocol=4)\n",
    "#     np.save(handle, all_predictions)\n",
    "#     np.save(handle, all_labels)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m121"
  },
  "kernelspec": {
   "display_name": "robustretfound_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
